{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecfd1cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python interpreter: c:\\Users\\kalpesh_sangle\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n",
      "Hello, virtual environment!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Using Python interpreter: {sys.executable}\")\n",
    "print(\"Hello, virtual environment!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update pip and install Hugging Face libraries\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers datasets evaluate accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250c7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26dc902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb7776d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kalpesh_sangle\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8b8033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.56.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: c:\\Users\\kalpesh_sangle\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b14cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79472536b0a1498daac19b087a2ca2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kalpesh_sangle\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kalpesh_sangle\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26461334c07b4efabed217139a37afd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94ba59c74734e818ae0f89e20b70e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cdc03eba62477fb8482cdaa1063996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: torch.Size([1, 7])\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model for sentiment analysis\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Example sentences for sentiment analysis\n",
    "inputs = tokenizer(\"I love Hugging Face!\", return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "outputs = model(**inputs)\n",
    "predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "# Print result\n",
    "labels = [\"Negative\", \"Positive\"]\n",
    "print(\"Text:\", inputs[\"input_ids\"].shape)\n",
    "print(\"Sentiment:\", labels[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a5c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love Hugging Face!\n",
      "Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "Text: This product is terrible.\n",
      "Sentiment: Negative\n",
      "--------------------------------------------------\n",
      "Text: I'm feeling great today!\n",
      "Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "Text: The weather is gloomy and depressing.\n",
      "Sentiment: Negative\n",
      "--------------------------------------------------\n",
      "Text: The movie was absolutely fantastic!\n",
      "Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "Text: The food was mediocre at best.\n",
      "Sentiment: Negative\n",
      "--------------------------------------------------\n",
      "Text: I'm so excited for the weekend.\n",
      "Sentiment: Positive\n",
      "--------------------------------------------------\n",
      "Text: I'm disappointed with the service.\n",
      "Sentiment: Negative\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# List of texts for sentiment analysis\n",
    "texts = [\n",
    "    \"I love Hugging Face!\",\n",
    "    \"This product is terrible.\",\n",
    "    \"I'm feeling great today!\",\n",
    "    \"The weather is gloomy and depressing.\",\n",
    "    \"The movie was absolutely fantastic!\",\n",
    "    \"The food was mediocre at best.\",\n",
    "    \"I'm so excited for the weekend.\",\n",
    "    \"I'm disappointed with the service.\"\n",
    "]\n",
    "\n",
    "# Perform sentiment analysis for each text\n",
    "for text in texts:\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Get model outputs\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted label\n",
    "    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    labels = [\"Negative\", \"Positive\"]  # DistilBERT uses SST-2 binary classification labels\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {labels[predicted_label]}\")\n",
    "    print(\"-\" * 50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c4ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
